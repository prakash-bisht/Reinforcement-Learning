{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "practice_reinforce.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84JnGzJqc2Yc"
      },
      "source": [
        "# REINFORCE in TensorFlow\n",
        "\n",
        "This notebook implements a basic reinforce algorithm a.k.a. policy gradient for CartPole env.\n",
        "\n",
        "It has been deliberately written to be as simple and human-readable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-gZcbS9c2Yr"
      },
      "source": [
        "The notebook assumes that you have [openai gym](https://github.com/openai/gym) installed.\n",
        "\n",
        "In case you're running on a server, [use xvfb](https://github.com/openai/gym#rendering-on-a-server)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LbJvkhqc2Yt",
        "outputId": "1c3a8ea5-b809-408f-ccbe-0fb3c724cd59"
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules:\n",
        "    %tensorflow_version 1.x\n",
        "    \n",
        "    if not os.path.exists('.setup_complete'):\n",
        "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "\n",
        "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
        "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n",
        "\n",
        "        !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 144865 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "SDKe-ZnWc2Yz",
        "outputId": "2d91d672-67b4-4d4e-f620-fd98d36a8752"
      },
      "source": [
        "import gym\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "#gym compatibility: unwrap TimeLimit\n",
        "if hasattr(env,'env'):\n",
        "    env=env.env\n",
        "\n",
        "x=env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "print(n_actions)\n",
        "print(state_dim)\n",
        "print(x)\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "(4,)\n",
            "[-0.00393026  0.01603155  0.04318051  0.04263353]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe8bf48d978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNYsHKmyc2Y2"
      },
      "source": [
        "# Building the policy network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrzeD3Ttc2Y3"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
        "\n",
        "For numerical stability, please __do not include the softmax layer into your network architecture__. \n",
        "\n",
        "We'll use softmax or log-softmax where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1cwNQkzc2Y8"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#create input variables. We only need <s,a,R> for REINFORCE\n",
        "states = tf.placeholder('float32',(None,)+state_dim,name=\"states\")\n",
        "actions = tf.placeholder('int32',name=\"action_ids\")\n",
        "cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OydSFhtac2Y-",
        "outputId": "a3a3d69b-49cf-4a97-8be5-8e17e6b7a30d"
      },
      "source": [
        "#<define network graph using raw tf or any deep learning library>\n",
        "import keras\n",
        "import keras.layers as L\n",
        "\n",
        "network = keras.models.Sequential()\n",
        "network.add(L.Dense(32, activation='relu', input_shape=state_dim))\n",
        "network.add(L.Dense(32, activation='relu'))\n",
        "network.add(L.Dense(n_actions, activation='linear'))\n",
        "\n",
        "logits = network(states)\n",
        "\n",
        "#####################\n",
        "policy = tf.nn.softmax(logits)\n",
        "log_policy = tf.nn.log_softmax(logits)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mr_6ML48c2ZB"
      },
      "source": [
        "#utility function to pick action in one given state\n",
        "get_action_proba = lambda s: policy.eval({states:[s]})[0] "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4csclN4cc2ZC"
      },
      "source": [
        "#### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
        "\n",
        "\n",
        "Following the REINFORCE algorithm, we can define our objective as follows: \n",
        "\n",
        "$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
        "\n",
        "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF_WeR6Xc2ZE"
      },
      "source": [
        "#get probabilities for parti\n",
        "indices = tf.stack([tf.range(tf.shape(log_policy)[0]),actions],axis=-1)\n",
        "log_policy_for_actions = tf.gather_nd(log_policy,indices)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW8ZgdRIc2ZN"
      },
      "source": [
        "# policy objective as in the last formula. please use mean, not sum.\n",
        "# note: you need to use log_policy_for_actions to get log probabilities for actions taken\n",
        "\n",
        "J = tf.reduce_mean(log_policy_for_actions*cumulative_rewards)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oHrNDBQc2ZO"
      },
      "source": [
        "#regularize with entropy\n",
        "entropy =  -tf.reduce_sum(policy * log_policy, 1, name=\"entropy\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIEC91Occ2ZP"
      },
      "source": [
        "#all network weights\n",
        "all_weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
        "\n",
        "#weight updates. maximizing J is same as minimizing -J. Adding negative entropy.\n",
        "loss = -J -0.1 * entropy\n",
        "\n",
        "update = tf.train.AdamOptimizer().minimize(loss,var_list=all_weights)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf2Yu4p-c2ZP"
      },
      "source": [
        "### Computing cumulative rewards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRPUntu_c2ZQ"
      },
      "source": [
        "from collections import deque\n",
        "def get_cumulative_rewards(rewards, #rewards at each step\n",
        "                           gamma = 0.99 #discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    take a list of immediate rewards r(s,a) for the whole session \n",
        "    compute cumulative rewards R(s,a) (a.k.a. G(s,a) in Sutton '16)\n",
        "    R_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "    \n",
        "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
        "    and compute R_t = r_t + gamma*R_{t+1} recurrently\n",
        "    \n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    \n",
        "    cumulative_rewards = deque([rewards[-1]])\n",
        "    for i in range(len(rewards)-2, -1, -1):\n",
        "        cumulative_rewards.appendleft(rewards[i]+gamma*cumulative_rewards[0])\n",
        "    \n",
        "        \n",
        "    return cumulative_rewards\n",
        "    \n",
        "    "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pm6Y_XU0c2ZR",
        "outputId": "0697d25f-34ff-4ba9-de7d-a15a688ca7b2"
      },
      "source": [
        "assert len(get_cumulative_rewards(range(100))) == 100\n",
        "assert np.allclose(get_cumulative_rewards([0,0,1,0,0,1,0],gamma=0.9),[1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(get_cumulative_rewards([0,0,1,-2,3,-4,0],gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(get_cumulative_rewards([0,0,1,2,3,4,0],gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "looks good!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgcp79bWc2ZS"
      },
      "source": [
        "def train_step(_states,_actions,_rewards):\n",
        "    \"\"\"given full session, trains agent with policy gradient\"\"\"\n",
        "    _cumulative_rewards = get_cumulative_rewards(_rewards)\n",
        "    update.run({states:_states,actions:_actions,cumulative_rewards:_cumulative_rewards})"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-3faZTnc2ZT"
      },
      "source": [
        "### Playing the game"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeFJORbNc2ZU"
      },
      "source": [
        "def generate_session(t_max=1000):\n",
        "    \"\"\"play env with REINFORCE agent and train at the session end\"\"\"\n",
        "    \n",
        "    #arrays to record session\n",
        "    states,actions,rewards = [],[],[]\n",
        "    \n",
        "    s = env.reset()\n",
        "    \n",
        "    for t in range(t_max):\n",
        "        \n",
        "        #action probabilities array aka pi(a|s)\n",
        "        action_probas = get_action_proba(s)\n",
        "        \n",
        "        a = np.random.choice(n_actions, 1, p=action_probas)[0]\n",
        "        \n",
        "        new_s,r,done,info = env.step(a)\n",
        "        \n",
        "        #record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "        \n",
        "        s = new_s\n",
        "        if done: break\n",
        "            \n",
        "    train_step(states,actions,rewards)\n",
        "            \n",
        "    return sum(rewards)\n",
        "        "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54R1ODQWc2ZV",
        "outputId": "0d147c79-b676-48c9-ad39-81e5f2ae69e9"
      },
      "source": [
        "s = tf.InteractiveSession()\n",
        "s.run(tf.global_variables_initializer())\n",
        "\n",
        "for i in range(100):\n",
        "    \n",
        "    rewards = [generate_session() for _ in range(100)] #generate new sessions\n",
        "    \n",
        "    print (\"mean reward:%.3f\"%(np.mean(rewards)))\n",
        "\n",
        "    if np.mean(rewards) > 300:\n",
        "        print (\"You Win!\")\n",
        "        break\n",
        "        \n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean reward:30.150\n",
            "mean reward:78.010\n",
            "mean reward:209.300\n",
            "mean reward:246.770\n",
            "mean reward:617.790\n",
            "You Win!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuexeuSGc2Zk"
      },
      "source": [
        "### Results & video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTKFEJQ-c2Zl",
        "outputId": "28c557a8-1414-492b-d7f1-4218b46eb1b4"
      },
      "source": [
        "#record sessions\n",
        "import gym.wrappers\n",
        "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
        "sessions = [generate_session() for _ in range(100)]\n",
        "env.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4jgoF69c2Zm",
        "outputId": "c3cbb06a-b372-4bac-bdd4-a232f6c4aaaf"
      },
      "source": [
        "#show video\n",
        "from IPython.display import HTML\n",
        "import os\n",
        "\n",
        "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<video width=\"640\" height=\"480\" controls>\n",
              "  <source src=\"./videos/openaigym.video.1.31125.video000001.mp4\" type=\"video/mp4\">\n",
              "</video>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8oq7H14c2Zm",
        "outputId": "550271af-1100-44be-8627-d51f3e7a7f2d"
      },
      "source": [
        "from submit import submit_cartpole\n",
        "submit_cartpole(generate_session, '', '')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Submitted to Coursera platform. See results on assignment page!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "gCO0tWVlc2Zo"
      },
      "source": [
        "# That's all, thank you for your attention!\n",
        "# Not having enough? There's an actor-critic waiting for you in the honor section.\n",
        "# But make sure you've seen the videos first."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}